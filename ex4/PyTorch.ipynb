{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last exercise we discussed the mechanisms of gradient backpropagation and implemented the corresponding functionality in the ``toolbox`` module. This exercise introduces PyTorch, a Deep Learning framework providing powerful tools for simple and efficient implementations of neural networks.\n",
    "\n",
    "First, we need to import the corresponding Python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic building block of PyTorch are Tensors, which behave very similar to the class ``toolbox.Tensor`` from last week. Tensors can be initialized by passing a NumPy array or by calling dedicated functions with syntax similar to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.array([[1., 2.], [3., 4.]]))\n",
    "b = torch.ones(2, 2)\n",
    "c = torch.empty(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch features the same mechanism for gradient backpropagation as developed in last week's exercise and even provides the same attribute names:\n",
    "- ``data`` stores the internal data of the tensor representing its value.\n",
    "- ``requires_grad`` specifies if the tensor participates in the build-up of the computation graph.\n",
    "- ``grad`` keeps a tensor storing the gradient of the last backpropagation for leaf nodes of the graph with ``requires_grad=True``.\n",
    "- ``grad_fn`` points to the operation generating the tensor; ``None`` if the tensor is a leaf node.\n",
    "\n",
    "Again, the backpropagation is initiated by calling ``backward`` on a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "d = c + a\n",
    "d = d * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(16.)\n",
      "<AddBackward0 object at 0x7f50c2cf3df0>\n",
      "<MulBackward0 object at 0x7f50c2cf38b0>\n"
     ]
    }
   ],
   "source": [
    "print(b.grad)\n",
    "print(a.grad)\n",
    "print(c.grad_fn)\n",
    "print(d.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, computation graphs will not be held alive causing errors when calling ``backward`` multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not working...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    d.backward()\n",
    "except:\n",
    "    print(\"Not working...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At inference time one usually does not want to bulild up the computation graph for calculating the gradients as it's computationally expensive and bears the danger of memory leakage. Therefore PyTorch provides the context manager ``torch.no_grad`` for disabling gradient computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throws a RuntimeError - Not working...\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2.)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c = a + b\n",
    "\n",
    "try:\n",
    "    c.backward()\n",
    "except Exception as e:\n",
    "    print(f\"Throws a {type(e).__name__} - Not working...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA [Only works on systems with a GPU - not required for the graded task]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, all computations in this exercise class were carried out on a CPU. The hardware of choice for Deep Learning applications, however, are GPUs. The high amount of parallelization and efficient calculation of simple operations on GPUs are highly beneficial for training and inference in artificial neural networks. Therefore, all major Deep Learning frameworks offer the possibility of utilizing GPUs.\n",
    "\n",
    "In PyTorch we can move a tensor to the GPU by simply calling the function ``cuda`` returning a pointer to the tensor. The device of a tensor can be determined using the ``device`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.)\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "a = a.cuda()\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, ``cuda`` means that the tensor is located on a GPU and the number after the colon specifies the device among all available GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are one driving force in the field of machine learning and therefore it is important to deal with data properly. Luckily, PyTorch provides some handy tools which make the treatment of data very easy. The base class for representing data sets is ``torch.utils.data.Dataset``. This class overloads the functions ``__len__`` and ``__getitem__`` which allow us to easily access data contained in the set as we know it for example from Pythons lists.\n",
    "\n",
    "PyTorch already comprises wrapper classes for some poular data sets in ``torchvision.datasets``. To get started, we will use the MNIST data set of handwritten digits from LeCun et al. http://yann.lecun.com/exdb/mnist/. The argument ``transform`` below forces the data to be converted to PyTorch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shindeshubhamm/anaconda3/envs/dl/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shindeshubhamm/anaconda3/envs/dl/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MNIST('data', download=True, transform=transforms.ToTensor())\n",
    "test_set = MNIST('data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single elements of these data sets are tuples containing an image representing a handwritten digit stored as an array in a PyTorch Tensor and the corresponding value of the digit as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "el = train_set[0]\n",
    "print(type(el))\n",
    "print(el[0].shape)\n",
    "print(el[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know the value of the digit represented in the image is 5. We can show the corresponding image using ``matplotlib.pyplot.imshow``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: 5')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(el[0][0], cmap='gray', vmin=0., vmax=1.)\n",
    "plt.title('Label: {}'.format(el[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``imshow`` requires grayscale images to be of size $H\\times W$ but ``el`` is of size $1\\times H \\times W$. The reason therefor is that images are in general represented as $C \\times H \\times W$ arrays, where $C$ denotes the number of channels of the image. For color images $C$ typically amounts to $3$, for grayscale images to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equally important as the data itself is the process of loading it efficiently to the processing unit. PyTorch provides an easy interface for tackling this problem. Most of the time, we also want mini-batches of data rather than single elements of our data set. The class ``DataLoader`` from ``torch.utils.data`` takes care of this and we only have to provide the data set and define the batch size. Additionally, we may specify further arguments like whether we want the data to be shuffled or the number of workers involved in the process of loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=4, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at one of the mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader), None)\n",
    "\n",
    "data = batch[0]\n",
    "labels = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(type(batch))\n",
    "print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the array has increased by the first dimension related to the differnt batch elements, i.e. the general shape for images in mini-batches is $B \\times C \\times H \\times W$ where $B$ is the batch size. Let's visualize the images of this mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(data[i, 0], cmap='gray', vmin=0., vmax=1.)\n",
    "    plt.title('Label: {}'.format(labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the elements are randomly shuffled each time we iterate over the data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our ``toolbox.Module``, PyTorch defines ``Module`` in ``torch.nn`` which serves as base class for neural networks and their layers. For defining a network, you have to inherit from ``torch.nn.Module`` and define the ``forward`` function. Furthermore, ``torch.nn`` provides you with a broad variety of pre-implemented tools and layers you can use in your network. To get a first impression, have a look at the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(28*28, 20)\n",
    "        self.layer2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(20, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28*28)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this: In the constructor we instantiate a couple of layers we want to use in the forward pass. For the linear layer we have to specifty the input and output dimensions of our linear transformations. The batch sizes usually are excluded from these size considerations as the data is procesed separately for each element in the mini-batch. To shed light on this mechanism, consider ``layer3``. It expects inputs of size $B \\times 20$, multiplies each element of the batch with a matrix of size $10 \\times 20$ (and possibly adds some vector afterwards) and outputs the transformed data of size $B \\times 10$.\n",
    "\n",
    "In order to be able to pass an image to a linear layer, we have to resize the image with the dimensions $B \\times C \\times H \\times W$ to a vector of size $B \\times \\left( C \\cdot H \\cdot W \\right)$. The forward pass of an object of type ``torch.nn.Module`` can be performed by simply calling the object itself like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = network(batch[0])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 4, 4])\n",
      "tensor([2, 1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "_, pred_labels = output.max(dim=1)\n",
    "\n",
    "print(pred_labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set the output of the last layer to be of size ten as we want to predict one out of ten labels. We would like our network to assign the highest value of the output on the coordinate corresponding to the correct label. As discussed earlier, the cross entropy loss is suitable for classification tasks penalizing high output values on the wrong labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4215)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fun(output, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as it comes to training, we want to modify the network's parameters in order to decrease the loss on the training set. We can access the learnable parameters of a network by calling its ``parameters`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = network.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 784])\n",
      "torch.Size([20])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in params:\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the parameters of the network are the weights and biases of the two linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding multiple layers to a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of layers increases, adding every single layer to the attribute list of the network and explicitly iterating over them in the forward pass can be tedious. A convenient alternative in this case is PyTorch's container class ``torch.nn.Sequential`` which allows you to condense multiple layers into a single ``Module`` object. The usage is straight-forward as you can see in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(28*28, 20), nn.ReLU(), nn.Linear(20, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** It is not sufficient to store the network's layers in a list as we did in our own ``Network`` class from the second last exercise class. Even though you would still be able to perform a forward pass, the layers are not registered as submodules of the network object and therefore their parameters can't be accessed using the ``parameters`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on GPU [Only works on systems with a GPU - not required for the graded task]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform training or inference of a network on a GPU we have to move the data as well as the network itself to the GPU. This again works by simply calling the ``cuda`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      2\u001b[0m network \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "data, labels = data.cuda(), labels.cuda()\n",
    "network = network.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = network(data)\n",
    "    loss = loss_fun(output, labels)\n",
    "\n",
    "print(output.device)\n",
    "print(loss.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[ 0.0056, -0.0261,  0.0046,  ..., -0.0300,  0.0339,  0.0341],\n",
       "                      [ 0.0148, -0.0089, -0.0253,  ...,  0.0333,  0.0135, -0.0075],\n",
       "                      [ 0.0249, -0.0154, -0.0122,  ..., -0.0048, -0.0082, -0.0019],\n",
       "                      ...,\n",
       "                      [ 0.0128,  0.0095, -0.0072,  ...,  0.0319, -0.0266,  0.0113],\n",
       "                      [-0.0148,  0.0269,  0.0286,  ..., -0.0324,  0.0083, -0.0034],\n",
       "                      [ 0.0300,  0.0245, -0.0143,  ...,  0.0326, -0.0242, -0.0162]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([-0.0018,  0.0092, -0.0084, -0.0247,  0.0322,  0.0048, -0.0233,  0.0063,\n",
       "                       0.0153,  0.0055,  0.0118, -0.0007, -0.0350,  0.0006,  0.0307,  0.0281,\n",
       "                      -0.0118,  0.0140,  0.0203,  0.0245])),\n",
       "             ('layer3.weight',\n",
       "              tensor([[-1.5334e-01,  6.5236e-02, -1.9632e-02,  1.1142e-05,  1.5201e-02,\n",
       "                        1.6231e-01, -1.0910e-01,  1.7135e-01, -9.0912e-03, -2.5841e-02,\n",
       "                       -1.4808e-01,  1.8489e-01,  4.4389e-02, -3.2402e-02, -6.4253e-02,\n",
       "                        4.3719e-02, -5.4292e-02,  1.4104e-01,  2.1933e-01, -2.1819e-02],\n",
       "                      [ 1.2963e-01,  1.6697e-01,  1.8145e-01,  5.3059e-02,  1.2096e-01,\n",
       "                       -2.1638e-02, -2.1557e-01, -2.1572e-01,  1.2291e-01,  1.7552e-01,\n",
       "                       -1.7800e-01, -3.4983e-02, -5.2794e-02, -1.4257e-01,  1.9993e-01,\n",
       "                        1.8128e-01, -3.8899e-02,  1.0799e-01, -5.9478e-02,  2.0138e-01],\n",
       "                      [ 6.5370e-02, -1.7321e-01, -2.5926e-02,  2.2238e-01, -1.3604e-01,\n",
       "                        1.5623e-01, -1.9521e-01, -2.1622e-01, -3.8962e-02,  1.1182e-01,\n",
       "                       -7.3623e-02, -7.4115e-02,  2.2058e-03,  2.0628e-01, -1.5600e-01,\n",
       "                       -1.8251e-01, -1.3577e-01,  6.2007e-02,  1.6719e-01,  1.6650e-01],\n",
       "                      [ 2.1273e-01, -1.9364e-01, -6.2784e-03,  2.5343e-02,  1.3802e-02,\n",
       "                       -9.8041e-02,  4.7078e-02, -6.9823e-02, -9.7994e-02, -1.5641e-01,\n",
       "                        1.6006e-01, -7.3149e-02,  4.0936e-02, -1.7028e-01,  5.6388e-02,\n",
       "                        1.5809e-01,  1.2519e-01,  1.9964e-01, -1.1556e-01, -1.9730e-01],\n",
       "                      [-1.9595e-01, -1.9840e-01,  1.2906e-01,  3.5603e-03,  9.3285e-02,\n",
       "                       -1.9153e-01,  1.8124e-01,  1.9940e-01,  1.6618e-01,  1.4218e-01,\n",
       "                        4.7117e-02, -1.1577e-01, -1.2924e-01,  7.6571e-02, -4.8819e-02,\n",
       "                        1.0902e-01,  6.7709e-02,  6.7990e-03, -3.1204e-02, -2.1074e-01],\n",
       "                      [-1.4319e-01,  8.1171e-02, -1.0193e-01,  1.5680e-01,  6.4485e-02,\n",
       "                        1.2691e-01,  7.4133e-02, -3.4251e-02,  9.3624e-04,  1.9741e-01,\n",
       "                       -8.0318e-02, -7.7039e-02, -1.0537e-01, -5.7524e-02,  3.9604e-02,\n",
       "                       -1.0335e-01,  4.8435e-02, -5.7283e-02, -1.2825e-01,  3.1148e-03],\n",
       "                      [-8.6980e-02, -1.1295e-01,  4.6692e-02,  1.0446e-02, -9.4766e-02,\n",
       "                        1.1684e-02, -1.0924e-01,  4.6710e-02,  1.5971e-01,  1.7640e-01,\n",
       "                       -1.4468e-01,  3.4124e-02,  1.3484e-01, -3.7387e-02, -1.4692e-01,\n",
       "                       -3.5887e-02,  4.4501e-02,  1.4385e-02, -7.1725e-02, -1.1624e-01],\n",
       "                      [ 2.3384e-02, -6.5253e-02, -3.7831e-02,  1.8703e-01,  2.0798e-01,\n",
       "                       -1.7562e-01, -9.4329e-02,  2.8828e-02,  1.6743e-01,  6.2342e-02,\n",
       "                        1.6054e-02,  7.3641e-02,  1.5161e-01,  1.5389e-01,  1.1698e-01,\n",
       "                       -1.8654e-01,  9.4536e-02, -1.1523e-01, -1.1523e-01, -8.0443e-03],\n",
       "                      [-8.8255e-02,  1.7626e-02,  4.4727e-02, -2.0713e-01, -8.7058e-02,\n",
       "                       -6.9467e-02,  4.5709e-02, -4.9720e-02, -5.2794e-02, -6.1636e-02,\n",
       "                        2.0481e-01, -6.1935e-04,  2.8485e-02, -6.4816e-02,  2.3524e-02,\n",
       "                        8.8809e-03, -5.5445e-02,  3.9877e-02,  1.3474e-01, -1.3085e-01],\n",
       "                      [ 2.0801e-01,  1.9184e-01, -2.1971e-01, -8.9679e-02, -1.8680e-01,\n",
       "                        1.8083e-01,  3.7173e-03, -1.6518e-01,  5.0837e-02, -1.6874e-01,\n",
       "                       -1.4459e-01, -6.6335e-02,  1.0979e-01,  1.3515e-01,  1.1506e-01,\n",
       "                       -1.7287e-01, -1.5394e-01,  1.6686e-01, -2.1878e-01,  1.6410e-01]])),\n",
       "             ('layer3.bias',\n",
       "              tensor([ 0.0956, -0.2177, -0.0230, -0.1009,  0.1885,  0.1853, -0.1503,  0.0839,\n",
       "                      -0.2180, -0.2083]))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0056, -0.0261,  0.0046,  ..., -0.0300,  0.0339,  0.0341],\n",
       "         [ 0.0148, -0.0089, -0.0253,  ...,  0.0333,  0.0135, -0.0075],\n",
       "         [ 0.0249, -0.0154, -0.0122,  ..., -0.0048, -0.0082, -0.0019],\n",
       "         ...,\n",
       "         [ 0.0128,  0.0095, -0.0072,  ...,  0.0319, -0.0266,  0.0113],\n",
       "         [-0.0148,  0.0269,  0.0286,  ..., -0.0324,  0.0083, -0.0034],\n",
       "         [ 0.0300,  0.0245, -0.0143,  ...,  0.0326, -0.0242, -0.0162]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0018,  0.0092, -0.0084, -0.0247,  0.0322,  0.0048, -0.0233,  0.0063,\n",
       "          0.0153,  0.0055,  0.0118, -0.0007, -0.0350,  0.0006,  0.0307,  0.0281,\n",
       "         -0.0118,  0.0140,  0.0203,  0.0245], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.5334e-01,  6.5236e-02, -1.9632e-02,  1.1142e-05,  1.5201e-02,\n",
       "           1.6231e-01, -1.0910e-01,  1.7135e-01, -9.0912e-03, -2.5841e-02,\n",
       "          -1.4808e-01,  1.8489e-01,  4.4389e-02, -3.2402e-02, -6.4253e-02,\n",
       "           4.3719e-02, -5.4292e-02,  1.4104e-01,  2.1933e-01, -2.1819e-02],\n",
       "         [ 1.2963e-01,  1.6697e-01,  1.8145e-01,  5.3059e-02,  1.2096e-01,\n",
       "          -2.1638e-02, -2.1557e-01, -2.1572e-01,  1.2291e-01,  1.7552e-01,\n",
       "          -1.7800e-01, -3.4983e-02, -5.2794e-02, -1.4257e-01,  1.9993e-01,\n",
       "           1.8128e-01, -3.8899e-02,  1.0799e-01, -5.9478e-02,  2.0138e-01],\n",
       "         [ 6.5370e-02, -1.7321e-01, -2.5926e-02,  2.2238e-01, -1.3604e-01,\n",
       "           1.5623e-01, -1.9521e-01, -2.1622e-01, -3.8962e-02,  1.1182e-01,\n",
       "          -7.3623e-02, -7.4115e-02,  2.2058e-03,  2.0628e-01, -1.5600e-01,\n",
       "          -1.8251e-01, -1.3577e-01,  6.2007e-02,  1.6719e-01,  1.6650e-01],\n",
       "         [ 2.1273e-01, -1.9364e-01, -6.2784e-03,  2.5343e-02,  1.3802e-02,\n",
       "          -9.8041e-02,  4.7078e-02, -6.9823e-02, -9.7994e-02, -1.5641e-01,\n",
       "           1.6006e-01, -7.3149e-02,  4.0936e-02, -1.7028e-01,  5.6388e-02,\n",
       "           1.5809e-01,  1.2519e-01,  1.9964e-01, -1.1556e-01, -1.9730e-01],\n",
       "         [-1.9595e-01, -1.9840e-01,  1.2906e-01,  3.5603e-03,  9.3285e-02,\n",
       "          -1.9153e-01,  1.8124e-01,  1.9940e-01,  1.6618e-01,  1.4218e-01,\n",
       "           4.7117e-02, -1.1577e-01, -1.2924e-01,  7.6571e-02, -4.8819e-02,\n",
       "           1.0902e-01,  6.7709e-02,  6.7990e-03, -3.1204e-02, -2.1074e-01],\n",
       "         [-1.4319e-01,  8.1171e-02, -1.0193e-01,  1.5680e-01,  6.4485e-02,\n",
       "           1.2691e-01,  7.4133e-02, -3.4251e-02,  9.3624e-04,  1.9741e-01,\n",
       "          -8.0318e-02, -7.7039e-02, -1.0537e-01, -5.7524e-02,  3.9604e-02,\n",
       "          -1.0335e-01,  4.8435e-02, -5.7283e-02, -1.2825e-01,  3.1148e-03],\n",
       "         [-8.6980e-02, -1.1295e-01,  4.6692e-02,  1.0446e-02, -9.4766e-02,\n",
       "           1.1684e-02, -1.0924e-01,  4.6710e-02,  1.5971e-01,  1.7640e-01,\n",
       "          -1.4468e-01,  3.4124e-02,  1.3484e-01, -3.7387e-02, -1.4692e-01,\n",
       "          -3.5887e-02,  4.4501e-02,  1.4385e-02, -7.1725e-02, -1.1624e-01],\n",
       "         [ 2.3384e-02, -6.5253e-02, -3.7831e-02,  1.8703e-01,  2.0798e-01,\n",
       "          -1.7562e-01, -9.4329e-02,  2.8828e-02,  1.6743e-01,  6.2342e-02,\n",
       "           1.6054e-02,  7.3641e-02,  1.5161e-01,  1.5389e-01,  1.1698e-01,\n",
       "          -1.8654e-01,  9.4536e-02, -1.1523e-01, -1.1523e-01, -8.0443e-03],\n",
       "         [-8.8255e-02,  1.7626e-02,  4.4727e-02, -2.0713e-01, -8.7058e-02,\n",
       "          -6.9467e-02,  4.5709e-02, -4.9720e-02, -5.2794e-02, -6.1636e-02,\n",
       "           2.0481e-01, -6.1935e-04,  2.8485e-02, -6.4816e-02,  2.3524e-02,\n",
       "           8.8809e-03, -5.5445e-02,  3.9877e-02,  1.3474e-01, -1.3085e-01],\n",
       "         [ 2.0801e-01,  1.9184e-01, -2.1971e-01, -8.9679e-02, -1.8680e-01,\n",
       "           1.8083e-01,  3.7173e-03, -1.6518e-01,  5.0837e-02, -1.6874e-01,\n",
       "          -1.4459e-01, -6.6335e-02,  1.0979e-01,  1.3515e-01,  1.1506e-01,\n",
       "          -1.7287e-01, -1.5394e-01,  1.6686e-01, -2.1878e-01,  1.6410e-01]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0956, -0.2177, -0.0230, -0.1009,  0.1885,  0.1853, -0.1503,  0.0839,\n",
       "         -0.2180, -0.2083], requires_grad=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(network.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a network using SGD [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD in ``optimizer.py`` as introduced in the lecture and do not use the SGD otimizer included in PyTorch. Test your implementation by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import SGD\n",
    "\n",
    "net = SimpleNet2()\n",
    "\n",
    "net.load_state_dict(torch.load('net_params'))\n",
    "\n",
    "batch = next(iter(DataLoader(train_set, batch_size=4)))\n",
    "\n",
    "op = net(batch[0])\n",
    "\n",
    "l = loss_fun(op, batch[1])\n",
    "\n",
    "l.backward()\n",
    "\n",
    "optim = SGD(net.parameters())\n",
    "\n",
    "optim.step()\n",
    "\n",
    "state_dict = net.state_dict()\n",
    "target_state_dict = torch.load('net_params_updated')\n",
    "\n",
    "for key in state_dict:\n",
    "    assert (state_dict[key] - target_state_dict[key]).abs().max() < 1e-6 # 1 point\n",
    "\n",
    "optim.zero_grad()\n",
    "\n",
    "for param in net.parameters():\n",
    "    assert param.grad.abs().max() == 0 # 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
